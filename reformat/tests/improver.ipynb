{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "from reformat import PromptReformatter\n",
    "from reformat import PromptImprover\n",
    "from reformat.rules import (\n",
    "    SeparatorRule,\n",
    "    CasingRule,\n",
    "    ItemFormattingRule,\n",
    "    EnumerationRule,\n",
    ")\n",
    "\n",
    "import logging\n",
    "\n",
    "# Disable httpx logging\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "\n",
    "# Disable reformat.improver logging \n",
    "logging.getLogger(\"reformat.improver\").setLevel(logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts = {\n",
    "    \"simple\": \"This is a test prompt\",\n",
    "    \"enumerated\": \"\"\"1. First item\n",
    "2. Second item\n",
    "3. Third item\"\"\",\n",
    "    \"fields\": \"\"\"Title: Test Document\n",
    "Author: John Doe\n",
    "Date: 2024-03-15\n",
    "Content: This is a test document with multiple fields.\"\"\",\n",
    "    \"complex\": \"\"\"TASK: Classify the sentiment\n",
    "INPUT: \"I absolutely love this product! Best purchase ever.\"\n",
    "OPTIONS:\n",
    "1. Positive\n",
    "2. Negative\n",
    "3. Neutral\n",
    "ANSWER:\"\"\",\n",
    "    \"instruction\": \"\"\"You are a helpful AI assistant.\n",
    "Follow these rules:\n",
    "1. Be concise\n",
    "2. Be accurate\n",
    "3. Be helpful\n",
    "Now answer the user's question:\n",
    "Q: What is the capital of France?\n",
    "A:\"\"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==================================================\n",
      "==================================================\n",
      "=== Testing Response Improvement ===\n",
      "\n",
      "Original Prompt:\n",
      "TASK: Classify the sentiment\n",
      "INPUT: \"I absolutely love this product! Best purchase ever.\"\n",
      "OPTIONS:\n",
      "1. Positive\n",
      "2. Negative\n",
      "3. Neutral\n",
      "ANSWER:\n",
      "\n",
      "Original Response:\n",
      "The sentiment of the input statement is clearly expressing enthusiasm and satisfaction with the product. \n",
      "\n",
      "ANSWER: 1. Positive\n",
      "\n",
      "Improved Response:\n",
      "The sentiment of the input statement is clearly expressing a strong positive emotion. The use of words like \"ABSOLUTELY LOVE\" and \"BEST PURCHASE EVER\" emphasizes the customer's satisfaction and enthusiasm for the product.\n",
      "\n",
      "ANSWER: <I>. POSITIVE\n",
      "\n",
      "Improvement Score: 0.800\n",
      "\n",
      "Best Format Used:\n",
      "{\n",
      "  \"separator_rule\": \"Double Colon\",\n",
      "  \"casing_rule\": \"Upper\",\n",
      "  \"item_formatting_rule\": \"Angle Brackets\",\n",
      "  \"enumeration_rule\": \"Roman\",\n",
      "  \"score\": 0.8\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def test_response_improvement(prompt: str, model: str = \"llama-3.3-70b-versatile\"):\n",
    "    \"\"\"Test how formatting affects response quality.\"\"\"\n",
    "    print(\"=== Testing Response Improvement ===\")\n",
    "    print(\"\\nOriginal Prompt:\")\n",
    "    print(prompt)\n",
    "\n",
    "    improver = PromptImprover(model=model)\n",
    "    result = improver.improve(\n",
    "        prompt,\n",
    "        num_candidates=5,  # Small number for testing\n",
    "        num_iterations=5,\n",
    "    )\n",
    "\n",
    "    print(\"\\nOriginal Response:\")\n",
    "    print(result[\"original_response\"])\n",
    "    print(\"\\nImproved Response:\")\n",
    "    print(result[\"improved_response\"])\n",
    "    print(f\"\\nImprovement Score: {result['improvement_score']:.3f}\")\n",
    "    print(\"\\nBest Format Used:\")\n",
    "    print(json.dumps(result[\"best_format\"], indent=2))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Test with different prompts\n",
    "prompt = test_prompts[\"complex\"]\n",
    "print(f\"\\n\\n{'=' * 50}\")\n",
    "print(\"=\" * 50)\n",
    "result = test_response_improvement(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing simple prompt ===\n",
      "\n",
      "Original:\n",
      "This is a test prompt\n",
      "\n",
      "Formatted:\n",
      "This is a test prompt\n",
      "\n",
      "=== Testing enumerated prompt ===\n",
      "\n",
      "Original:\n",
      "1. First item\n",
      "2. Second item\n",
      "3. Third item\n",
      "\n",
      "Formatted:\n",
      "(1). First item\n",
      "(2). Second item\n",
      "(3). Third item\n",
      "\n",
      "=== Testing fields prompt ===\n",
      "\n",
      "Original:\n",
      "Title: Test Document\n",
      "Author: John Doe\n",
      "Date: 2024-03-15\n",
      "Content: This is a test document with multiple fields.\n",
      "\n",
      "Formatted:\n",
      "Title: Test Document\n",
      "Author: John Doe\n",
      "Date: (2)0(2)(4)-0(3)-(1)(5)\n",
      "Content: This is a test document with multiple fields.\n",
      "\n",
      "=== Testing complex prompt ===\n",
      "\n",
      "Original:\n",
      "TASK: Classify the sentiment\n",
      "INPUT: \"I absolutely love this product! Best purchase ever.\"\n",
      "OPTIONS:\n",
      "1. Positive\n",
      "2. Negative\n",
      "3. Neutral\n",
      "ANSWER:\n",
      "\n",
      "Formatted:\n",
      "TASK: Classify the sentiment\n",
      "INPUT: \"I absolutely love this product! Best purchase ever.\"\n",
      "OPTIONS:\n",
      "(1). Positive\n",
      "(2). Negative\n",
      "(3). Neutral\n",
      "ANSWER:\n",
      "\n",
      "=== Testing instruction prompt ===\n",
      "\n",
      "Original:\n",
      "You are a helpful AI assistant.\n",
      "Follow these rules:\n",
      "1. Be concise\n",
      "2. Be accurate\n",
      "3. Be helpful\n",
      "Now answer the user's question:\n",
      "Q: What is the capital of France?\n",
      "A:\n",
      "\n",
      "Formatted:\n",
      "You are a helpful AI assistant.\n",
      "Follow these rules:\n",
      "(1). Be concise\n",
      "(2). Be accurate\n",
      "(3). Be helpful\n",
      "Now answer the user's question:\n",
      "Q: What is the capital of France?\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "# Create reformatter\n",
    "reformatter = PromptReformatter()\n",
    "\n",
    "# Test formatting on different prompt types\n",
    "for prompt_type, prompt in test_prompts.items():\n",
    "    print(f\"\\n=== Testing {prompt_type} prompt ===\")\n",
    "    print(\"\\nOriginal:\")\n",
    "    print(prompt)\n",
    "    print(\"\\nFormatted:\")\n",
    "    print(reformatter.format(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Double Newline + Title ===\n",
      "Title: Test Document\n",
      "\n",
      "Author: John Doe\n",
      "\n",
      "Date: (2)0(2)(4)-0(3)-(1)(5)\n",
      "\n",
      "Content: This Is A Test Document With Multiple Fields.\n",
      "\n",
      "=== Testing Double Newline + Upper ===\n",
      "TITLE: TEST DOCUMENT\n",
      "\n",
      "AUTHOR: JOHN DOE\n",
      "\n",
      "DATE: (2)0(2)(4)-0(3)-(1)(5)\n",
      "\n",
      "CONTENT: THIS IS A TEST DOCUMENT WITH MULTIPLE FIELDS.\n",
      "\n",
      "=== Testing Dash + Title ===\n",
      "Title: Test Document - Author: John Doe - Date: (2)0(2)(4)-0(3)-(1)(5) - Content: This Is A Test Document With Multiple Fields.\n",
      "\n",
      "=== Testing Dash + Upper ===\n",
      "TITLE: TEST DOCUMENT - AUTHOR: JOHN DOE - DATE: (2)0(2)(4)-0(3)-(1)(5) - CONTENT: THIS IS A TEST DOCUMENT WITH MULTIPLE FIELDS.\n"
     ]
    }
   ],
   "source": [
    "separator_rules = [\n",
    "    SeparatorRule(\"Double Newline\", \"Use double newline\", \"\\n\\n\"),\n",
    "    SeparatorRule(\"Dash\", \"Use dash separator\", \" - \"),\n",
    "]\n",
    "\n",
    "casing_rules = [\n",
    "    CasingRule(\"Title\", \"Use title case\"),\n",
    "    CasingRule(\"Upper\", \"Use uppercase\"),\n",
    "]\n",
    "\n",
    "# Test different combinations\n",
    "for separator in separator_rules:\n",
    "    for casing in casing_rules:\n",
    "        print(f\"\\n=== Testing {separator.name} + {casing.name} ===\")\n",
    "        reformatter = PromptReformatter(\n",
    "            separator_rules=[separator], casing_rules=[casing]\n",
    "        )\n",
    "        formatted = reformatter.format(test_prompts[\"fields\"])\n",
    "        print(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:reformat.improver:Starting iteration 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing with llama-3.3-70b-versatile ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Testing with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m improver \u001b[38;5;241m=\u001b[39m PromptImprover(model\u001b[38;5;241m=\u001b[39mmodel, api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGROQ_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 13\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mimprover\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimprove\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_candidates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Small number for testing\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Single iteration for testing\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal Prompt:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_prompt)\n",
      "File \u001b[0;32m/private/var/git/anlp-prompt-formatting/reformat/reformat/improver.py:265\u001b[0m, in \u001b[0;36mPromptImprover.improve\u001b[0;34m(self, prompt, num_candidates, num_iterations, temperature)\u001b[0m\n\u001b[1;32m    262\u001b[0m model_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model_response(formatted_prompt)\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# 3. Evaluate the format\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatted_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m candidate\u001b[38;5;241m.\u001b[39mscore \u001b[38;5;241m=\u001b[39m score\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Cache the score\u001b[39;00m\n",
      "File \u001b[0;32m/private/var/git/anlp-prompt-formatting/reformat/reformat/improver.py:192\u001b[0m, in \u001b[0;36mPromptImprover.evaluate_format\u001b[0;34m(self, original_prompt, formatted_prompt, model_response)\u001b[0m\n\u001b[1;32m    160\u001b[0m         evaluation_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mEvaluate the effectiveness of this prompt formatting.\u001b[39m\n\u001b[1;32m    161\u001b[0m \n\u001b[1;32m    162\u001b[0m \u001b[38;5;124mOriginal Prompt:\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m \n\u001b[1;32m    182\u001b[0m \u001b[38;5;124mProvide only the numerical score (e.g., 0.85) without any explanation.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    184\u001b[0m         messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    185\u001b[0m             {\n\u001b[1;32m    186\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: evaluation_prompt},\n\u001b[1;32m    190\u001b[0m         ]\n\u001b[0;32m--> 192\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjudge_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m             score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mstrip())\n",
      "File \u001b[0;32m/private/var/git/anlp-prompt-formatting/.venv/lib/python3.11/site-packages/aisuite/client.py:117\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, model, messages, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load provider for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprovider_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Delegate the chat completion to the correct provider's implementation\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprovider\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_completions_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/private/var/git/anlp-prompt-formatting/.venv/lib/python3.11/site-packages/aisuite/providers/groq_provider.py:22\u001b[0m, in \u001b[0;36mGroqProvider.chat_completions_create\u001b[0;34m(self, model, messages, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat_completions_create\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, messages, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass any additional arguments to the Groq API\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/private/var/git/anlp-prompt-formatting/.venv/lib/python3.11/site-packages/groq/resources/chat/completions.py:298\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    187\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    188\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/openai/v1/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/private/var/git/anlp-prompt-formatting/.venv/lib/python3.11/site-packages/groq/_base_client.py:1263\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1251\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1258\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1259\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1260\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1261\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1262\u001b[0m     )\n\u001b[0;32m-> 1263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/private/var/git/anlp-prompt-formatting/.venv/lib/python3.11/site-packages/groq/_base_client.py:955\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    953\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 955\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/private/var/git/anlp-prompt-formatting/.venv/lib/python3.11/site-packages/groq/_base_client.py:1058\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1055\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1057\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1061\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1062\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1067\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}"
     ]
    }
   ],
   "source": [
    "target_models = [\n",
    "    \"llama-3.3-70b-versatile\",  # High quality but slower\n",
    "    \"llama-3.3-8b-instant\",  # Faster but lower quality\n",
    "]\n",
    "\n",
    "test_prompt = test_prompts[\"complex\"]  # Use the complex prompt for testing\n",
    "\n",
    "for model in target_models:\n",
    "    print(f\"\\n=== Testing with {model} ===\")\n",
    "\n",
    "    improver = PromptImprover(model=model, api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "    result = improver.improve(\n",
    "        test_prompt,\n",
    "        num_candidates=3,  # Small number for testing\n",
    "        num_iterations=1,  # Single iteration for testing\n",
    "        temperature=0.1,\n",
    "    )\n",
    "\n",
    "    print(\"\\nOriginal Prompt:\")\n",
    "    print(test_prompt)\n",
    "    print(\"\\nBest Formatted Prompt:\")\n",
    "    print(result[\"improved_prompt\"])\n",
    "    print(\"\\nModel Response:\")\n",
    "    print(result[\"model_response\"])\n",
    "    print(f\"\\nFormat Score: {result['improvement_score']:.3f}\")\n",
    "    print(\"\\nBest Format:\")\n",
    "    print(json.dumps(result[\"best_format\"], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:reformat.reformat.improver:Starting iteration 1/1\n",
      "INFO:reformat.reformat.improver:New best score: 0.587\n",
      "INFO:reformat.reformat.improver:New best score: 0.723\n",
      "INFO:reformat.reformat.improver:Starting iteration 1/1\n",
      "INFO:reformat.reformat.improver:New best score: 0.326\n",
      "INFO:reformat.reformat.improver:New best score: 0.688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First run (no cache):\n",
      "Candidates evaluated: 2\n",
      "\n",
      "Second run (should use cache):\n",
      "Candidates evaluated: 2\n"
     ]
    }
   ],
   "source": [
    "improver = PromptImprover()\n",
    "prompt = test_prompts[\"simple\"]\n",
    "\n",
    "print(\"First run (no cache):\")\n",
    "result1 = improver.improve(prompt, num_iterations=1, num_candidates=2)\n",
    "print(f\"Candidates evaluated: {result1['num_candidates_evaluated']}\")\n",
    "\n",
    "print(\"\\nSecond run (should use cache):\")\n",
    "result2 = improver.improve(prompt, num_iterations=1, num_candidates=2)\n",
    "print(f\"Candidates evaluated: {result2['num_candidates_evaluated']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing format performance on complex prompt:\n",
      "Top 5 performing formats:\n",
      "\n",
      "1. Score: 0.954\n",
      "{\n",
      "  \"separator_rule\": \"Double Colon\",\n",
      "  \"casing_rule\": \"Title\",\n",
      "  \"item_formatting_rule\": \"Brackets\",\n",
      "  \"enumeration_rule\": \"Roman\",\n",
      "  \"score\": null\n",
      "}\n",
      "\n",
      "2. Score: 0.785\n",
      "{\n",
      "  \"separator_rule\": \"Dash\",\n",
      "  \"casing_rule\": \"Lower\",\n",
      "  \"item_formatting_rule\": \"Parentheses\",\n",
      "  \"enumeration_rule\": \"Roman\",\n",
      "  \"score\": null\n",
      "}\n",
      "\n",
      "3. Score: 0.739\n",
      "{\n",
      "  \"separator_rule\": \"Space\",\n",
      "  \"casing_rule\": \"Title\",\n",
      "  \"item_formatting_rule\": \"Angle Brackets\",\n",
      "  \"enumeration_rule\": \"Numeric\",\n",
      "  \"score\": null\n",
      "}\n",
      "\n",
      "4. Score: 0.725\n",
      "{\n",
      "  \"separator_rule\": \"Double Colon\",\n",
      "  \"casing_rule\": \"No Change\",\n",
      "  \"item_formatting_rule\": \"Parentheses\",\n",
      "  \"enumeration_rule\": \"Alpha\",\n",
      "  \"score\": null\n",
      "}\n",
      "\n",
      "5. Score: 0.719\n",
      "{\n",
      "  \"separator_rule\": \"Newline\",\n",
      "  \"casing_rule\": \"No Change\",\n",
      "  \"item_formatting_rule\": \"Parentheses\",\n",
      "  \"enumeration_rule\": \"Numeric\",\n",
      "  \"score\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def run_format_analysis(prompt, num_trials=50):\n",
    "    improver = PromptImprover()\n",
    "    format_scores = []\n",
    "\n",
    "    for _ in range(num_trials):\n",
    "        candidate = improver.sample_candidate()\n",
    "        formatted = improver.format_prompt(prompt, candidate)\n",
    "        score = improver.evaluate_format(prompt, formatted)\n",
    "        format_scores.append({\"format\": candidate.to_dict(), \"score\": score})\n",
    "\n",
    "    # Sort by score\n",
    "    format_scores.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "    print(\"Top 5 performing formats:\")\n",
    "    for i, result in enumerate(format_scores[:5], 1):\n",
    "        print(f\"\\n{i}. Score: {result['score']:.3f}\")\n",
    "        print(json.dumps(result[\"format\"], indent=2))\n",
    "\n",
    "\n",
    "# Run analysis on complex prompt\n",
    "print(\"Analyzing format performance on complex prompt:\")\n",
    "run_format_analysis(test_prompts[\"complex\"], num_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_format_test(prompt):\n",
    "    \"\"\"Interactive function to test different format combinations.\"\"\"\n",
    "    reformatter = PromptReformatter()\n",
    "\n",
    "    # Get all available rules\n",
    "    separator_rules = SeparatorRule.get_default_rules()\n",
    "    casing_rules = CasingRule.get_default_rules()\n",
    "    item_rules = ItemFormattingRule.get_default_rules()\n",
    "    enum_rules = EnumerationRule.get_default_rules()\n",
    "\n",
    "    # Print available rules\n",
    "    print(\"Available Rules:\")\n",
    "    print(\"\\nSeparator Rules:\", [r.name for r in separator_rules])\n",
    "    print(\"Casing Rules:\", [r.name for r in casing_rules])\n",
    "    print(\"Item Formatting Rules:\", [r.name for r in item_rules])\n",
    "    print(\"Enumeration Rules:\", [r.name for r in enum_rules])\n",
    "\n",
    "    # Format with selected rules\n",
    "    selected_separator = separator_rules[0]  # Change index to test different rules\n",
    "    selected_casing = casing_rules[0]  # Change index to test different rules\n",
    "\n",
    "    reformatter = PromptReformatter(\n",
    "        separator_rules=[selected_separator], casing_rules=[selected_casing]\n",
    "    )\n",
    "\n",
    "    print(\"\\nOriginal Prompt:\")\n",
    "    print(prompt)\n",
    "    print(f\"\\nFormatted with {selected_separator.name} + {selected_casing.name}:\")\n",
    "    print(reformatter.format(prompt))\n",
    "\n",
    "\n",
    "# Test interactive formatting\n",
    "test_prompt = test_prompts[\"instruction\"]\n",
    "interactive_format_test(test_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
